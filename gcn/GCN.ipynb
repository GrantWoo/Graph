{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "\n",
    "config = {\n",
    "    'dataset': 'cora',\n",
    "    'hidden1': 16,\n",
    "    'epochs': 200,\n",
    "    'early_stopping': 20,\n",
    "    'weight_decay': 5e-4,\n",
    "    'learning_rate': 0.01,\n",
    "    'dropout': 0.,\n",
    "    'verbose': False,\n",
    "    'logging': False,\n",
    "    'gpu_id': None\n",
    "}\n",
    "\n",
    "FLAGS = EasyDict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 辅助函数\n",
    "\n",
    "## 数据读取与处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_planetoid(dataset):\n",
    "    \"\"\" Load dataset of the splitted version from Planetoid\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: string\n",
    "        name of the dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A_mat,\n",
    "    X_mat,\n",
    "    z_vec,\n",
    "    idx_train,\n",
    "    idx_val,\n",
    "    idx_test\n",
    "    \"\"\"\n",
    "    if dataset not in ['citeseer', 'cora', 'pubmed']:\n",
    "        print(\"No dataset found!\")\n",
    "    keys = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = defaultdict()\n",
    "    for key in keys:\n",
    "        with open('data_split/ind.{}.{}'.format(dataset, key), 'rb') as f:\n",
    "            objects[key] = pickle.load(f, encoding='latin1')\n",
    "    test_index = [int(x) for x in open('data_split/ind.{}.test.index'.format(dataset))]\n",
    "    test_index_sort = np.sort(test_index)\n",
    "    G = nx.from_dict_of_lists(objects['graph'])\n",
    "\n",
    "    A_mat = nx.adjacency_matrix(G)\n",
    "    X_mat = sp.vstack((objects['allx'], objects['tx'])).tolil()\n",
    "    X_mat[test_index, :] = X_mat[test_index_sort, :]\n",
    "    z_vec = np.vstack((objects['ally'], objects['ty']))\n",
    "    z_vec[test_index, :] = z_vec[test_index_sort, :]\n",
    "    z_vec = z_vec.argmax(1)\n",
    "\n",
    "    train_idx = range(len(objects['y']))\n",
    "    val_idx = range(len(objects['y']), len(objects['y']) + 500)\n",
    "    test_idx = test_index_sort.tolist()\n",
    "\n",
    "    return A_mat, X_mat, z_vec, train_idx, val_idx, test_idx\n",
    "\n",
    "\n",
    "def preprocess_graph(adj, c=1):\n",
    "    \"\"\" process the graph\n",
    "        * options:\n",
    "        normalization of augmented adjacency matrix\n",
    "        formulation from convolutional filter\n",
    "        normalized graph laplacian\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adj: a sparse matrix represents the adjacency matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adj_normalized: a sparse matrix represents the normalized laplacian\n",
    "        matrix\n",
    "    \"\"\"\n",
    "    _adj = adj + c * sp.eye(adj.shape[0])  # Sparse matrix with ones on diagonal 产生对角矩阵\n",
    "    # _D = sp.diags(_dseq)\n",
    "    _dseq = _adj.sum(1).A1  # 按行求和后拉直\n",
    "    _D_half = sp.diags(np.power(_dseq, -0.5)) # 开平方构成对角矩阵\n",
    "    adj_normalized = _D_half @ _adj @ _D_half # 矩阵乘法\n",
    "    return adj_normalized.tocsr()    # 转成稀疏矩阵存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{D}^{-0.5} \\hat{A} \\hat{D}^{-0.5}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_graph(A_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=bool, numpy=\n",
       "array([False,  True, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A_mat\n",
    "t = tf.random.uniform([20])+0.2\n",
    "tf.cast(tf.floor(t), dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于处理稀疏矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_dropout(x, dropout_rate, noise_shape):\n",
    "    \"\"\"\n",
    "    在SparseTensor保留指定非空值。\n",
    "    x 输入的稀疏矩阵\n",
    "    noise_shape：x中的元素个数\n",
    "    \n",
    "    \"\"\"\n",
    "    random_tensor = 1 - dropout_rate\n",
    "    random_tensor += tf.random.uniform(noise_shape) # 一维数组(noise_shape,)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool) # bool型 一维数组(noise_shape,)\n",
    "    pre_out = tf.sparse.retain(x, dropout_mask) # 根据dropout_mask的True False 保留x的数据\n",
    "    return pre_out * (1. / (1 - dropout_rate)) # 数字处理\n",
    "\n",
    "\n",
    "def sp_matrix_to_sp_tensor(M):\n",
    "    \"\"\"\n",
    "    tf.SparseTensor的作用是构造一个稀疏矩阵类，便于为其他的API提供输入(稀疏矩阵的输入)。\n",
    "    returns\n",
    "    X包含 indices,values,dense_shape,dtype\n",
    "    \"\"\"\n",
    "    if not isinstance(M, sp.csr.csr_matrix):\n",
    "        M = M.tocsr()\n",
    "    row, col = M.nonzero()  # 非零元素的行和列\n",
    "    X = tf.SparseTensor(np.mat([row, col]).T, M.data, M.shape) # 函数用于将输入解释为矩阵\n",
    "    X = tf.cast(X, tf.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义图卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, regularizers, constraints, initializers\n",
    "spdot = tf.sparse.sparse_dense_matmul\n",
    "dot = tf.matmul\n",
    "\n",
    "\n",
    "class GCNConv(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 activation=lambda x: x,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_initializer='zeros',\n",
    "                 bias_regularizer=None,\n",
    "                 bias_constraint=None,\n",
    "                 activity_regularizer=None,\n",
    "                 **kwargs):\n",
    "        # 初始化不需要训练的参数\n",
    "        self.units = units\n",
    "        # activation=None 使用线性激活函数（等价不使用激活函数）\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # 初始化方法定义了对Keras层设置初始化权重（bias）的方法 glorot_uniform\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        \n",
    "        # 加载正则化的方法\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        \n",
    "        # 约束：对权重值施加约束的函数。\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(GCNConv, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\" GCN has two inputs : [shape(An), shape(X)]\n",
    "        \"\"\"\n",
    "        # gsize = input_shape[0][0]  # graph size\n",
    "        fdim = input_shape[1][1]  # feature dim\n",
    "        \n",
    "        # hasattr 检查该对象self是否有某个属性'weight'\n",
    "        if not hasattr(self, 'weight'):\n",
    "            self.weight = self.add_weight(name=\"weight\",\n",
    "                                          shape=(fdim, self.units),\n",
    "                                          initializer=self.kernel_initializer,\n",
    "                                          constraint=self.kernel_constraint,\n",
    "                                          trainable=True)\n",
    "        if self.use_bias:\n",
    "            if not hasattr(self, 'bias'):\n",
    "                self.bias = self.add_weight(name=\"bias\",\n",
    "                                            shape=(self.units, ),\n",
    "                                            initializer=self.bias_initializer,\n",
    "                                            constraint=self.bias_constraint,\n",
    "                                            trainable=True)\n",
    "        super(GCNConv, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\" GCN has two inputs : [An, X]\n",
    "        \"\"\"\n",
    "        self.An = inputs[0]\n",
    "        self.X = inputs[1]\n",
    "        # isinstance 函数来判断一个对象是否是一个已知的类型\n",
    "        if isinstance(self.X, tf.SparseTensor):\n",
    "            h = spdot(self.X, self.weight)\n",
    "        else:\n",
    "            # 二维数组矩阵之间的dot函数运算得到的乘积是矩阵乘积\n",
    "            h = dot(self.X, self.weight)\n",
    "        output = spdot(self.An, h)\n",
    "\n",
    "        if self.use_bias:\n",
    "            output = tf.nn.bias_add(output, self.bias)\n",
    "\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义GCN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdot = tf.sparse.sparse_dense_matmul\n",
    "dot = tf.matmul\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "class GCN():\n",
    "    def __init__(self, An, X, sizes, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        An : scipy.sparse matrix\n",
    "            normalized adjacency matrix\n",
    "        X : scipy.sparse matrix\n",
    "            feature matrix\n",
    "        sizes : list  # [16,7]\n",
    "            size in each layer\n",
    "        \"\"\"\n",
    "        # 初始化参数\n",
    "        self.with_relu = True\n",
    "        self.with_bias = True\n",
    "\n",
    "        self.lr = FLAGS.learning_rate\n",
    "        self.dropout = FLAGS.dropout\n",
    "        self.verbose = FLAGS.verbose\n",
    "        \n",
    "        self.An = An\n",
    "        self.X = X\n",
    "        self.layer_sizes = sizes\n",
    "        self.shape = An.shape\n",
    "        \n",
    "        # 预处理数据\n",
    "        self.An_tf = sp_matrix_to_sp_tensor(self.An)\n",
    "        self.X_tf = sp_matrix_to_sp_tensor(self.X)\n",
    "        \n",
    "        # 初始化要用到的层和优化器\n",
    "        self.layer1 = GCNConv(self.layer_sizes[0], activation='relu')\n",
    "        self.layer2 = GCNConv(self.layer_sizes[1])\n",
    "        self.opt = tf.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "    # \n",
    "    def train(self, idx_train, labels_train, idx_val, labels_val):\n",
    "        \"\"\" Train the model\n",
    "        idx_train : array like\n",
    "        labels_train : array like\n",
    "        \"\"\"\n",
    "        K = labels_train.max() + 1\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        # use adam to optimize\n",
    "        for it in range(FLAGS.epochs):\n",
    "            tic = time()\n",
    "            with tf.GradientTape() as tape:\n",
    "                _loss = self.loss_fn(idx_train, np.eye(K)[labels_train])\n",
    "\n",
    "            # optimize over weights\n",
    "            grad_list = tape.gradient(_loss, self.var_list)\n",
    "            grads_and_vars = zip(grad_list, self.var_list)\n",
    "            self.opt.apply_gradients(grads_and_vars)\n",
    "\n",
    "            # evaluate on the training\n",
    "            train_loss, train_acc = self.evaluate(idx_train, labels_train, training=True)\n",
    "            train_losses.append(train_loss)\n",
    "            val_loss, val_acc = self.evaluate(idx_val, labels_val, training=False)\n",
    "            val_losses.append(val_loss)\n",
    "            toc = time()\n",
    "            if self.verbose:\n",
    "                print(\"iter:{:03d}\".format(it),\n",
    "                      \"train_loss:{:.4f}\".format(train_loss),\n",
    "                      \"train_acc:{:.4f}\".format(train_acc),\n",
    "                      \"val_loss:{:.4f}\".format(val_loss),\n",
    "                      \"val_acc:{:.4f}\".format(val_acc),\n",
    "                      \"time:{:.4f}\".format(toc - tic))\n",
    "        return train_losses\n",
    "\n",
    "    def loss_fn(self, idx, labels, training=True):\n",
    "        \"\"\" Calculate the loss function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : array like\n",
    "        labels : array like\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _loss : scalar\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            # X_tf：X_mat sparse_tensor的矩阵,X.nnz: X中一共多少个元素\n",
    "            _X = sparse_dropout(self.X_tf, self.dropout, [self.X.nnz])\n",
    "        else:\n",
    "            _X = self.X_tf\n",
    "\n",
    "        self.h1 = self.layer1([self.An_tf, _X])\n",
    "        if training:\n",
    "            _h1 = tf.nn.dropout(self.h1, self.dropout)\n",
    "        else:\n",
    "            _h1 = self.h1\n",
    "\n",
    "        self.h2 = self.layer2([self.An_tf, _h1])\n",
    "        self.var_list = self.layer1.weights + self.layer2.weights\n",
    "        # calculate the loss base on idx and labels\n",
    "        # tf.gather 根据索引从参数轴收集切片。\n",
    "        _logits = tf.gather(self.h2, idx)\n",
    "        _loss_per_node = tf.nn.softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                                 logits=_logits)\n",
    "        _loss = tf.reduce_mean(_loss_per_node)\n",
    "        # the weight_dacay only applys to the first layer.\n",
    "        # weight decay(权值衰减)的作用是调节模型复杂度（体现在 self.layer1.weights）对损失函数的影响，若weight decay很大，则复杂的模型损失函数的值也就大。\n",
    "        #         Same as the original implementation of GCN.\n",
    "        # _loss += FLAGS.weight_decay * sum(map(tf.nn.l2_loss, self.var_list))\n",
    "        # map(fun,series): 根据提供的函数对指定序列做映射。\n",
    "        # tf.nn.l2_loss: sum(t ** 2) / 2\n",
    "        _loss += FLAGS.weight_decay * sum(map(tf.nn.l2_loss, self.layer1.weights))\n",
    "        return _loss\n",
    "\n",
    "    def evaluate(self, idx, true_labels, training):\n",
    "        \"\"\" Evaluate the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : array like\n",
    "        true_labels : true labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _loss : scalar\n",
    "        _acc : scalar\n",
    "        \"\"\"\n",
    "        K = true_labels.max() + 1\n",
    "        _loss = self.loss_fn(idx, np.eye(K)[true_labels], training=training).numpy()\n",
    "        _pred_logits = tf.gather(self.h2, idx)\n",
    "        _pred_labels = tf.argmax(_pred_logits, axis=1).numpy()\n",
    "        _acc = accuracy_score(_pred_labels, true_labels)\n",
    "        return _loss, _acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<2708x2708 sparse matrix of type '<class 'numpy.intc'>'\n",
       " \twith 10556 stored elements in Compressed Sparse Row format>,\n",
       " <2708x1433 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 49216 stored elements in List of Lists format>,\n",
       " array([3, 4, 4, ..., 3, 3, 3], dtype=int64),\n",
       " range(0, 140),\n",
       " range(140, 640),\n",
       " [1708, 1709, 1710, 1711, 1712])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) == 0 or FLAGS.gpu_id is None:\n",
    "    device_id = \"/device:CPU:0\"\n",
    "else:\n",
    "    tf.config.experimental.set_visible_devices(gpus[FLAGS.gpu_id], 'GPU')\n",
    "    device_id = '/device:GPU:0'\n",
    "\n",
    "A_mat, X_mat, z_vec, train_idx, val_idx, test_idx = load_data_planetoid(FLAGS.dataset)\n",
    "A_mat, X_mat, z_vec, train_idx, val_idx, test_idx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x2708 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13264 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "An_mat = preprocess_graph(A_mat)\n",
    "An_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x1433 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 49216 stored elements in List of Lists format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_idx\n",
    "# 小功能1\n",
    "# K = z_vec.max()+1  # 数字7，用于指定维度\n",
    "# labels_train = z_vec[train_idx] # 一维数组，用于指定哪个位置为1\n",
    "# np.eye(K)[labels_train][:2]\n",
    "\n",
    "# 保留一定的数据\n",
    "# dropout_rate = 0.2\n",
    "# noise_shape = [X_mat.nnz]\n",
    "# x = sp_matrix_to_sp_tensor(X_mat)\n",
    "# random_tensor = 1 - dropout_rate\n",
    "# random_tensor += tf.random.uniform(noise_shape) # 一维数组(noise_shape,)\n",
    "# dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool) # bool型 一维数组(noise_shape,)\n",
    "# pre_out = tf.sparse.retain(x, dropout_mask) # 根据dropout_mask的True False 保留x的数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# N = A_mat.shape[0]\n",
    "K = z_vec.max() + 1\n",
    "\n",
    "with tf.device(device_id): # 指定模型运行的具体设备，可以指定运行在GPU还是CUP上，以及哪块GPU上。\n",
    "    gcn = GCN(An_mat, X_mat, [FLAGS.hidden1, K])  # hidden1=16， out_put: K = 7\n",
    "    gcn.train(train_idx, z_vec[train_idx], val_idx, z_vec[val_idx])\n",
    "    test_res = gcn.evaluate(test_idx, z_vec[test_idx], training=False)\n",
    "    # gcn = GCN(An_mat_diag, X_mat_stack, [FLAGS.hidden1, K])\n",
    "    # gcn.train(train_idx_recal, z_vec[train_idx], val_idx_recal, z_vec[val_idx])\n",
    "    # test_res = gcn.evaluate(test_idx_recal, z_vec[test_idx], training=False)\n",
    "    print(\"Dataset {}\".format(FLAGS.dataset),\n",
    "          \"Test loss {:.4f}\".format(test_res[0]),\n",
    "          \"test acc {:.4f}\".format(test_res[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cora Test loss 0.6549 test acc 0.8160\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # config the CPU/GPU in TF, assume only one GPU is in use.\n",
    "    # For multi-gpu setting, please refer to\n",
    "    #   https://www.tensorflow.org/guide/gpu#using_multiple_gpus\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if len(gpus) == 0 or FLAGS.gpu_id is None:\n",
    "        device_id = \"/device:CPU:0\"\n",
    "    else:\n",
    "        tf.config.experimental.set_visible_devices(gpus[FLAGS.gpu_id], 'GPU')\n",
    "        device_id = '/device:GPU:0'\n",
    "\n",
    "    A_mat, X_mat, z_vec, train_idx, val_idx, test_idx = load_data_planetoid(FLAGS.dataset)\n",
    "    An_mat = preprocess_graph(A_mat)\n",
    "\n",
    "    # N = A_mat.shape[0]\n",
    "    K = z_vec.max() + 1\n",
    "\n",
    "    with tf.device(device_id):\n",
    "        gcn = GCN(An_mat, X_mat, [FLAGS.hidden1, K])\n",
    "        gcn.train(train_idx, z_vec[train_idx], val_idx, z_vec[val_idx])\n",
    "        test_res = gcn.evaluate(test_idx, z_vec[test_idx], training=False)\n",
    "        # gcn = GCN(An_mat_diag, X_mat_stack, [FLAGS.hidden1, K])\n",
    "        # gcn.train(train_idx_recal, z_vec[train_idx], val_idx_recal, z_vec[val_idx])\n",
    "        # test_res = gcn.evaluate(test_idx_recal, z_vec[test_idx], training=False)\n",
    "        print(\"Dataset {}\".format(FLAGS.dataset),\n",
    "              \"Test loss {:.4f}\".format(test_res[0]),\n",
    "              \"test acc {:.4f}\".format(test_res[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
